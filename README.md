This project focuses on building a Question Answering (QA) system for low-resource languages using the mT5-small multilingual transformer model. The goal is to enable accurate question answering even when limited annotated data is available.

The system leverages the multilingual capabilities of mT5-small to understand questions and generate relevant answers from given contexts. It is particularly useful for languages that lack large-scale QA datasets. The project demonstrates data preprocessing, fine-tuning, and evaluation of a transformer-based QA model using Python and deep learning libraries.

This work highlights the effectiveness of transfer learning and multilingual models in addressing challenges faced in low-resource NLP settings.
